{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm_writer_hw.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rSmJJczZo5zp"},"source":["# 專題（一）：訓練LSTM之歌詞自動填詞器\n","\n","## 專案目標\n","- 目標：使用 LSTM 模型去學習五月天歌詞，並且可以自動填詞來產生歌詞\n","- mayday_lyrics.txt 資料說明：\n","    - 每一行都是一首歌的歌詞\n","    - 除去標點符號並以空白表示間格\n","- 利用 mayday_lyrics.txt 來產生歌詞的序列\n","- 使用 LSTM 模型去學習歌詞的序列\n","- 當我們給定開頭的一段歌詞，例如：”給我一首歌”，就可以用 LSTM 猜下一個字，反覆這個過程就可以自動填詞\n","\n","## 實作提示\n","- STEP1：從 mayday_lyrics.txt 中取出歌詞\n","- STEP2：建立每個字的 Index\n","- STEP3：用 Rolling 的方式打造 LyricsDataset\n","- STEP4：使用 DataLoader 來包裝 LyricsDataset\n","- STEP5：建立 LSTM 模型： inputs > nn.Embedding > nn.LSTM > nn.Dropout > 取最後一個 state > nn.Linear > softmax\n","- STEP6：開始訓練並調整參數\n","- STEP7：進行 Demo，給定 pre_text ，使用模型迭代的預測下一個字產生歌詞\n","- (進階) STEP8：在 Demo 時可以採用依照 Softmax 機率來作隨機採樣，這可以增加隨機性，讓歌詞有更多變化，當然你還可以使用機率閥值來避免太奇怪的字出現\n","\n","## 重要知識點：專題結束後你可以學會\n","- 如何讀取並處理需要 Rolling 的序列資料\n","- 了解如何用 Pytorch 建制一個 LSTM 的模型\n","- 學會如何訓練一個語言模型\n","- 學會如何隨機抽樣自 Softmax 的分布"]},{"cell_type":"code","metadata":{"id":"hen1MQ1F_cly"},"source":["import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data import random_split\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7joX8NEEu90J"},"source":["# from: https://github.com/gaussic/Chinese-Lyric-Corpus\n","\n","lyrics_list = [line.strip() for line in open('mayday_lyrics.txt')]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_P3Bonv_7FpS"},"source":["# 建立詞典對照表\n","word2index = {}\n","index2word = {}\n","\n","i = 0\n","for words in lyrics_list:\n","    for word in words:\n","        if word not in word2index:\n","            word2index[word] = i\n","            index2word[i] = word\n","            i += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nueOEx287Hpm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c6660bfe-a513-424d-95bd-a8b8400255d0"},"source":["len(word2index)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2101"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"BwIpjwU_8YJB"},"source":["# 建立數據集\n","class LyricsDataset(Dataset):\n","    def __init__(self, lyrics_list, word2index, num_unrollings=10):\n","        ## Code Here\n","\n","    def __getitem__(self, idx):\n","        ## Code Here\n","\n","    def __len__(self):\n","        return len(self.samples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_L7UokS6_W7O"},"source":["batch_size = 128\n","\n","dataset = LyricsDataset(lyrics_list, word2index)\n","\n","train_loader = DataLoader(\n","    dataset=dataset,\n","    batch_size=batch_size,\n","    shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_HV3bcPM_l7X"},"source":["# 建立模型\n","class LM_LSTM(nn.Module):\n","    def __init__(self, n_hidden, vocab_size, num_layers, dropout_ratio):\n","        super(LM_LSTM, self).__init__()\n","        ## Code Here\n","\n","    def forward(self, inputs):\n","        ## Code Here\n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tNgfswV_nc1"},"source":["def train_batch(model, data, criterion, optimizer, device):\n","    model.train()\n","    inputs, targets = [d.to(device) for d in data]\n","\n","    outputs = model(inputs)\n","\n","    loss = criterion(outputs, targets)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    return loss.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_QVIJqHRsnX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ad0be24-37a2-463c-90db-206ca71f8251"},"source":["# 訓練模型\n","epochs = 100\n","lr = 0.001\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = LM_LSTM(128, len(word2index), 2, 0.2)\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss(size_average=False)\n","criterion.to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","\n","for epoch in range(1, 1 + epochs):\n","    tot_train_loss = 0\n","    tot_train_count = 0\n","\n","    for train_data in train_loader:\n","        loss = train_batch(model, train_data, criterion, optimizer, device)\n","\n","        tot_train_loss += loss\n","        tot_train_count += train_data[0].size(0)\n","\n","    print('epoch ', epoch, 'train_loss: ', tot_train_loss / tot_train_count)\n","\n","    if epoch % 10 == 0:\n","        for idx in [0, 50, 99]:\n","            input_batch = dataset[idx][0].unsqueeze(0).to(device)\n","            predict = model(input_batch).argmax(dim=-1).item()\n","            print('Example: \"{}\"+\"{}\"'.format(dataset.samples[idx][:-1], index2word[predict]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["epoch  1 train_loss:  5.638955304613609\n","epoch  2 train_loss:  5.200973100893721\n","epoch  3 train_loss:  4.884452973550726\n","epoch  4 train_loss:  4.619896265843904\n","epoch  5 train_loss:  4.386603825503506\n","epoch  6 train_loss:  4.173271787438875\n","epoch  7 train_loss:  3.9685163989069663\n","epoch  8 train_loss:  3.7773809852131452\n","epoch  9 train_loss:  3.5979248391393033\n","epoch  10 train_loss:  3.430616509478376\n","Example: \"摸不到的顏色 是否\"+\"唱\"\n","Example: \" 只留下結果 時間\"+\"有\"\n","Example: \"麼多的燦爛的夢 以\"+\"為\"\n","epoch  11 train_loss:  3.273799685966239\n","epoch  12 train_loss:  3.122795784916941\n","epoch  13 train_loss:  2.9793283374761645\n","epoch  14 train_loss:  2.845751559415817\n","epoch  15 train_loss:  2.7248610255581696\n","epoch  16 train_loss:  2.6080863793743774\n","epoch  17 train_loss:  2.4989614634187785\n","epoch  18 train_loss:  2.391229770826993\n","epoch  19 train_loss:  2.2937631835545518\n","epoch  20 train_loss:  2.202239961635645\n","Example: \"摸不到的顏色 是否\"+\"叫\"\n","Example: \" 只留下結果 時間\"+\"有\"\n","Example: \"麼多的燦爛的夢 以\"+\"為\"\n","epoch  21 train_loss:  2.1240416840813823\n","epoch  22 train_loss:  2.0374399667942558\n","epoch  23 train_loss:  1.9673210294652947\n","epoch  24 train_loss:  1.9024232826681557\n","epoch  25 train_loss:  1.8347291509263879\n","epoch  26 train_loss:  1.7716841393623042\n","epoch  27 train_loss:  1.7125604754944366\n","epoch  28 train_loss:  1.6571562784794687\n","epoch  29 train_loss:  1.6051265854279952\n","epoch  30 train_loss:  1.5560110039348765\n","Example: \"摸不到的顏色 是否\"+\"叫\"\n","Example: \" 只留下結果 時間\"+\"又\"\n","Example: \"麼多的燦爛的夢 以\"+\"為\"\n","epoch  31 train_loss:  1.5111155620777141\n","epoch  32 train_loss:  1.4620676138145086\n","epoch  33 train_loss:  1.4252789378730757\n","epoch  34 train_loss:  1.3875592859572607\n","epoch  35 train_loss:  1.349162942808256\n","epoch  36 train_loss:  1.3195752783548123\n","epoch  37 train_loss:  1.2830185644791985\n","epoch  38 train_loss:  1.2492924716862066\n","epoch  39 train_loss:  1.2185312500766299\n","epoch  40 train_loss:  1.1855852836610354\n","Example: \"摸不到的顏色 是否\"+\"叫\"\n","Example: \" 只留下結果 時間\"+\"分\"\n","Example: \"麼多的燦爛的夢 以\"+\"為\"\n","epoch  41 train_loss:  1.1520344671678004\n","epoch  42 train_loss:  1.140727493131873\n","epoch  43 train_loss:  1.105119613104285\n","epoch  44 train_loss:  1.0852029711768976\n","epoch  45 train_loss:  1.0620739163129638\n","epoch  46 train_loss:  1.0383545788619843\n","epoch  47 train_loss:  1.0215694635214343\n","epoch  48 train_loss:  0.9932833109378123\n","epoch  49 train_loss:  0.9721032659566204\n","epoch  50 train_loss:  0.9565134218954343\n","Example: \"摸不到的顏色 是否\"+\"叫\"\n","Example: \" 只留下結果 時間\"+\"輪\"\n","Example: \"麼多的燦爛的夢 以\"+\"為\"\n","epoch  51 train_loss:  0.9387705089299171\n","epoch  52 train_loss:  0.928245118769163\n","epoch  53 train_loss:  0.9156579849152234\n","epoch  54 train_loss:  0.8865967279353575\n","epoch  55 train_loss:  0.8747419650503451\n","epoch  56 train_loss:  0.863197462237469\n","epoch  57 train_loss:  0.8492010783980845\n","epoch  58 train_loss:  0.8314484935370715\n","epoch  59 train_loss:  0.8229394911836433\n","epoch  60 train_loss:  0.8101496092748978\n","Example: \"摸不到的顏色 是否\"+\"阻\"\n","Example: \" 只留下結果 時間\"+\"偷\"\n","Example: \"麼多的燦爛的夢 以\"+\"為\"\n","epoch  61 train_loss:  0.7960867087560172\n","epoch  62 train_loss:  0.7843096271255438\n","epoch  63 train_loss:  0.7673354735172043\n","epoch  64 train_loss:  0.7685243239483684\n","epoch  65 train_loss:  0.7483435013609081\n","epoch  66 train_loss:  0.7366173506895531\n","epoch  67 train_loss:  0.7323001300063867\n","epoch  68 train_loss:  0.7210601823419046\n","epoch  69 train_loss:  0.7103907538629441\n","epoch  70 train_loss:  0.7076701916762188\n","Example: \"摸不到的顏色 是否\"+\"叫\"\n","Example: \" 只留下結果 時間\"+\"期\"\n","Example: \"麼多的燦爛的夢 以\"+\"為\"\n","epoch  71 train_loss:  0.6918934242408261\n","epoch  72 train_loss:  0.6824665106557603\n","epoch  73 train_loss:  0.6783401396411936\n","epoch  74 train_loss:  0.670836842722768\n","epoch  75 train_loss:  0.6636410296799709\n","epoch  76 train_loss:  0.6522231044825618\n","epoch  77 train_loss:  0.6431907762687092\n","epoch  78 train_loss:  0.6403991860278264\n","epoch  79 train_loss:  0.6272548684836696\n","epoch  80 train_loss:  0.6245843236174684\n","Example: \"摸不到的顏色 是否\"+\"叫\"\n","Example: \" 只留下結果 時間\"+\"偷\"\n","Example: \"麼多的燦爛的夢 以\"+\"為\"\n","epoch  81 train_loss:  0.6157866119473895\n","epoch  82 train_loss:  0.6111026945576699\n","epoch  83 train_loss:  0.6147282287183408\n","epoch  84 train_loss:  0.6023901444547398\n","epoch  85 train_loss:  0.5995152212186309\n","epoch  86 train_loss:  0.5917026581672642\n","epoch  87 train_loss:  0.5834617247345742\n","epoch  88 train_loss:  0.5830875535422356\n","epoch  89 train_loss:  0.5709787450804105\n","epoch  90 train_loss:  0.5704593549993748\n","Example: \"摸不到的顏色 是否\"+\"叫\"\n","Example: \" 只留下結果 時間\"+\"偷\"\n","Example: \"麼多的燦爛的夢 以\"+\"為\"\n","epoch  91 train_loss:  0.5704819733782833\n","epoch  92 train_loss:  0.5627415502691978\n","epoch  93 train_loss:  0.5509866551701434\n","epoch  94 train_loss:  0.5510440663039937\n","epoch  95 train_loss:  0.560093838755692\n","epoch  96 train_loss:  0.549618619548223\n","epoch  97 train_loss:  0.5308916659418744\n","epoch  98 train_loss:  0.5362993082605118\n","epoch  99 train_loss:  0.530826860190567\n","epoch  100 train_loss:  0.5300889618587625\n","Example: \"摸不到的顏色 是否\"+\"叫\"\n","Example: \" 只留下結果 時間\"+\"偷\"\n","Example: \"麼多的燦爛的夢 以\"+\"為\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"anMER7TJTWKy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8ad0434c-91ee-4542-cc3a-bf881d2fb379"},"source":["# 模型inference\n","pre_text = '給我一首歌'\n","generate_len = 50\n","prob_threshold = 0.01\n","\n","result = [word2index[c] for c in pre_text]\n","for _ in range(generate_len):\n","    input_example = torch.tensor([result], dtype=torch.long, device=device)\n","    logit = model(input_example)\n","\n","    ## Code Here\n","\n","    ## End\n","    result += [predict]\n","print(''.join([index2word[i] for i in result]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["給我一首歌的火 勇果我不斷不想要給你 一千個小快 每一分攏都樂的了在心風 遺憾的感動 是你的永然在幽上 天使像\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NeJ2a5VM8OPM"},"source":[""],"execution_count":null,"outputs":[]}]}