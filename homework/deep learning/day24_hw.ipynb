{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhOs0IrWW1-E"
      },
      "source": [
        "## NMT English to Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_RByKlJWo1I"
      },
      "source": [
        "## 載入torch 0.6.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLryd8VynrYz",
        "outputId": "e40f44da-7257-4760-c7df-b0edf9563f82"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 23.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 30.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 22.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 25.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 24.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 27.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.8.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.9.0\n",
            "    Uninstalling torchtext-0.9.0:\n",
            "      Successfully uninstalled torchtext-0.9.0\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc6fyby3WxIb"
      },
      "source": [
        "## 匯入package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5pj0lwocq8R"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import spacy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J78VW_MxdI7Q",
        "outputId": "31976913-0f25-4690-cd2d-3ba52ef93053"
      },
      "source": [
        "!pip install matplotlib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0VkC2DPcvlv",
        "outputId": "bccbf262-f259-4b8e-af0d-952d5c1e66c2"
      },
      "source": [
        "\n",
        "# Colab 進行matplotlib繪圖時顯示繁體中文\n",
        "# 下載字體並命名taipei_sans_tc_beta.ttf，移至指定路徑\n",
        "!wget -O taipei_sans_tc_beta.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
        "!mv taipei_sans_tc_beta.ttf /usr/local/lib/python3.7/dist-packages/matplotlib//mpl-data/fonts/ttf\n",
        "\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import matplotlib.pyplot as plt \n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "import matplotlib.ticker as ticker\n",
        "# 自定義字體變數\n",
        "myfont = FontProperties(fname=r'/usr/local/lib/python3.7/dist-packages/matplotlib/mpl-data/fonts/ttf/taipei_sans_tc_beta.ttf')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-25 11:51:45--  https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.15.110, 2607:f8b0:4004:811::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.15.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hqci9hvngth6jlcbf69ne7egs23ddtma/1616673075000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_ [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-03-25 11:51:47--  https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hqci9hvngth6jlcbf69ne7egs23ddtma/1616673075000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)... 172.217.13.225, 2607:f8b0:4004:809::2001\n",
            "Connecting to doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)|172.217.13.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-font-ttf]\n",
            "Saving to: ‘taipei_sans_tc_beta.ttf’\n",
            "\n",
            "taipei_sans_tc_beta     [  <=>               ]  19.70M  39.7MB/s    in 0.5s    \n",
            "\n",
            "2021-03-25 11:51:48 (39.7 MB/s) - ‘taipei_sans_tc_beta.ttf’ saved [20659344]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIF1UdY_dm-g",
        "outputId": "2bae9c96-efa1-43e9-cac3-85734a9e1e25"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96vuG4eddqNq",
        "outputId": "c92d25e4-4a25-436b-ddcd-596ae883c9c1"
      },
      "source": [
        "data_dir = '/content/drive/My Drive/data/'\n",
        "lines = open(data_dir + 'cmn.txt' , encoding='utf-8').read().strip().split('\\n')\n",
        "trnslt_pairs = [[s for s in l.split('\\t')] for l in lines ]\n",
        "print (\"Sample: \" , trnslt_pairs[1000][:2] )\n",
        "print (\"Total records:\" , len(trnslt_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample:  ['He was drowned.', '他被淹死了。']\n",
            "Total records: 24360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCQhVzxBeb4L",
        "outputId": "2373e623-8bf1-4771-df16-dd3234732d62"
      },
      "source": [
        "# create train and validation set\n",
        "\n",
        "print (\"Total records after filtering :\" , len(trnslt_pairs))\n",
        "train, test = train_test_split(trnslt_pairs, test_size=0.1)\n",
        "train, val = train_test_split(train, test_size=0.1)\n",
        "print (\"training data:{} , develop data: {} , testing data: {}\".format(len(train),len(val),len(test)))\n",
        "    \n",
        "def write_csv(trn_data, file_path ):\n",
        "    with open(file_path ,'w', newline='', encoding='utf-8') as fout:\n",
        "        writer = csv.writer (fout)\n",
        "        for itm in trn_data: \n",
        "            writer.writerow ([itm[0],itm[1]])\n",
        "            \n",
        "file_path = data_dir + 'train.csv'\n",
        "write_csv(train, file_path )\n",
        "\n",
        "file_path = data_dir + 'val.csv'\n",
        "write_csv(val, file_path )\n",
        "    \n",
        "file_path = data_dir + 'test.csv'\n",
        "write_csv(test, file_path )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total records after filtering : 24360\n",
            "training data:19731 , develop data: 2193 , testing data: 2436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCWkf8che_x6"
      },
      "source": [
        "# 下載 spacy 的英文模型 幫我們做tokenize\n",
        "spacy_eng = spacy.load('en_core_web_sm')\n",
        "def tokenize_eng(text):\n",
        "  #清除不需要的字符\n",
        "  text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "  return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "def tokenize_cmn(text):\n",
        "  #去掉非中文字元\n",
        "  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
        "  text = regex.sub(' ', text)\n",
        "\n",
        "  return [word for word in text if word.strip()]\n",
        "\n",
        "\n",
        "TRG = Field(tokenize = tokenize_eng, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "SRC = Field(tokenize = tokenize_cmn, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            include_lengths = True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHZS3UvifctB",
        "outputId": "5b6b388b-9222-4988-b4f4-fb84af687310"
      },
      "source": [
        "# 讀取csv用TabularDataset\n",
        "train_dataset, dev_dataset, test_dataset = TabularDataset.splits(\n",
        "    path = data_dir , format = 'csv', skip_header = True,\n",
        "    train='train.csv', validation='val.csv', test='test.csv',\n",
        "    fields=[\n",
        "        ('trg', TRG),\n",
        "        ('src', SRC)\n",
        "    ]\n",
        ")\n",
        "SRC.build_vocab(train_dataset, min_freq = 1)\n",
        "TRG.build_vocab(train_dataset, min_freq = 1)\n",
        "\n",
        "print (\"中文語料的字元表長度: \" , len(SRC.vocab) , \", 英文的字元表長度: \" ,len(TRG.vocab))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "中文語料的字元表長度:  3353 , 英文的字元表長度:  6092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLgojEXCflOa"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dataset, dev_dataset, test_dataset), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.src),\n",
        "     device = device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oqp6B8VXfmB"
      },
      "source": [
        "* encoder_outputs 就是input每個hidden的輸出 假設有此句有4個詞，hidden dim為32 batch_size 為 2 那就是[4, 2, 32*2(bidirect\n",
        ")]\n",
        "* hidden 因為是要餵給decoder，所以用的是decoder的大小，此題是encode_hidden的2倍[2, 64]\n",
        "* mask 之後補"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VZFvX2mOLz_"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        # 這邊開始，同學可以跟隨我們的建議 完成程式 或是自行寫作\n",
        "        # 整理代表 q and k 的 hidden and encoder_output \n",
        "        \n",
        "        # [batch size, 1, dec hid dim]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        # 計算 向量拼接方式 ATTENTION\n",
        "        # 先將 q and k concat 起來\n",
        "        # 然後經過一層 W attention 自學參數,\n",
        "        # 和一個 tanh activation function.\n",
        "        # 最後乘以一個 Vt 調整成一個同等於input seq 的數列.\n",
        "        cat_item = torch.cat((hidden, encoder_outputs), 2)\n",
        "\n",
        "        # print(cat_item.shape)\n",
        "        attn_cat_item = self.attn(cat_item)\n",
        "        # print(attn_cat_item.shape)\n",
        "        attention = torch.tanh(attn_cat_item)\n",
        "        # print(attention.shape)\n",
        "        #attention = [batch size, src len, dec hid dim]\n",
        "        # 將 ATTENTION 結果乘以 V\n",
        "        attention = self.v(attention)\n",
        "        attention=attention.squeeze(2)\n",
        "        # print(attention.shape)\n",
        "        \n",
        "        #attention = [batch size, src len]\n",
        "        # apply MASK 建議使用 tensor 的 masked_fill \n",
        "        attention = attention.masked_fill(mask==0, -1e10)\n",
        "        \n",
        "        return F.softmax(attention, dim = 1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NfXO9RzdNa7"
      },
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        input_dim: 3349\n",
        "        emb_dim: 300\n",
        "        enc_hid_dim: 256\n",
        "        dec_hid_dim: 512\n",
        "        dropout: 0.5\n",
        "        \"\"\"\n",
        "        # 我們的例子:[3349, 300]\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        # 雙向 ＧＲＵ  [300, 1, 256*2]\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        # [512, 512]\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        \"\"\"\n",
        "        src: [src len, batch size]  包起來的樣子[每一個batch的max_src_len, batch_szie]\n",
        "        src_len: [batch size]\n",
        "        \"\"\"\n",
        "\n",
        "        # [src len, batch size, emb dim]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "                \n",
        "        # 使用pack_padded_sequence 來壓縮序列        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "\n",
        "        # 使用 pad_packed_sequence 用來展開序列成原本形狀的      \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "            \n",
        "        #outputs shape [src len, batch size, hid dim * num directions]\n",
        "        #hidden shape [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden 堆疊 [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs 是最後一層 \n",
        "        \n",
        "        #hidden [-2, :, : ] 是最後一層 forwards RNN \n",
        "        #hidden [-1, :, : ] 是最後一層 backwards RNN\n",
        "        \n",
        "        # hidden 是最後再過一層 dense layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shm4X6vrlWG2"
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        # 單向 ＧＲＵ decoder \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        # 512 + 512 + 256\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "             \n",
        "        #input shape [batch size]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        #encoder_outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #mask shape [batch size, src len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input shape [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded shape [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "        # print(a)\n",
        "                \n",
        "        #a shape [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a shape [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs shape [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted shape [batch size, 1, enc hid dim * 2]\n",
        "        # a * value\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted shape [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input shape [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output shape [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden shape [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output shape [1, batch size, dec hid dim]\n",
        "        #hidden shape [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction shape [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7wrBbAJie6C"
      },
      "source": [
        "class Seq2SeqATTN(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "                    \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        #mask = [batch size, src len]\n",
        "                \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
        "            #  and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmxSwtLs8zBq",
        "outputId": "e7a18dc3-7a69-4c64-f414-e2ab2238784d"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 256 # 注意 encoder hidden layer 設定 必須為 dec 的一半 \n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2SeqATTN(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "\n",
        "def initial_mdl_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(initial_mdl_weights)\n",
        "print (\"模型全部參數量: {:10,d} \".format(sum(p.numel() for p in model.parameters())))\n",
        "model"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "模型全部參數量: 13,768,396 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqATTN(\n",
              "  (encoder): RNNEncoder(\n",
              "    (embedding): Embedding(3353, 256)\n",
              "    (rnn): GRU(256, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): RNNDecoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(6092, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=6092, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNJnW0Sh86uH"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src, src_len = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, src_len.cpu() , trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, src_len.cpu(), trg, 0) #turn off teacher forcing\n",
        "            \n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLxQagjM9tc9",
        "outputId": "3f4da564-c4f1-4177-b983-35b8ce2b602e"
      },
      "source": [
        "MAX_EPOCHS = 20\n",
        "CLIP = 2\n",
        "model_dir =  '/content/drive/My Drive/data/'\n",
        "best_valid_loss = 9999999\n",
        "\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    torch.save(model.state_dict(), model_dir + 'model-{}.pt'.format(epoch))\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), model_dir + 'best-model.pt')\n",
        "   \n",
        "    print (\"Epoch {} training time: {:.2f} sec Training Loss: {:.3f} , Valiation Loss: {:.3f}\".format( epoch , end_time - start_time , train_loss , valid_loss))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 training time: 15.69 sec Training Loss: 5.399 , Valiation Loss: 5.026\n",
            "Epoch 1 training time: 15.79 sec Training Loss: 4.542 , Valiation Loss: 4.733\n",
            "Epoch 2 training time: 15.83 sec Training Loss: 4.116 , Valiation Loss: 4.549\n",
            "Epoch 3 training time: 16.12 sec Training Loss: 3.714 , Valiation Loss: 4.208\n",
            "Epoch 4 training time: 16.13 sec Training Loss: 3.325 , Valiation Loss: 3.983\n",
            "Epoch 5 training time: 16.07 sec Training Loss: 2.993 , Valiation Loss: 3.837\n",
            "Epoch 6 training time: 15.90 sec Training Loss: 2.665 , Valiation Loss: 3.714\n",
            "Epoch 7 training time: 15.88 sec Training Loss: 2.340 , Valiation Loss: 3.637\n",
            "Epoch 8 training time: 16.27 sec Training Loss: 2.053 , Valiation Loss: 3.618\n",
            "Epoch 9 training time: 15.99 sec Training Loss: 1.832 , Valiation Loss: 3.539\n",
            "Epoch 10 training time: 15.84 sec Training Loss: 1.648 , Valiation Loss: 3.523\n",
            "Epoch 11 training time: 15.93 sec Training Loss: 1.508 , Valiation Loss: 3.475\n",
            "Epoch 12 training time: 15.97 sec Training Loss: 1.327 , Valiation Loss: 3.563\n",
            "Epoch 13 training time: 15.85 sec Training Loss: 1.232 , Valiation Loss: 3.560\n",
            "Epoch 14 training time: 15.92 sec Training Loss: 1.119 , Valiation Loss: 3.584\n",
            "Epoch 15 training time: 15.99 sec Training Loss: 1.038 , Valiation Loss: 3.613\n",
            "Epoch 16 training time: 15.99 sec Training Loss: 0.928 , Valiation Loss: 3.697\n",
            "Epoch 17 training time: 15.82 sec Training Loss: 0.849 , Valiation Loss: 3.729\n",
            "Epoch 18 training time: 15.82 sec Training Loss: 0.786 , Valiation Loss: 3.770\n",
            "Epoch 19 training time: 15.81 sec Training Loss: 0.713 , Valiation Loss: 3.844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqtVAjSE-5hb"
      },
      "source": [
        "# Save SRC and TRG vocab\n",
        "torch.save(SRC.vocab, model_dir + 'SRC_vocab.pt')\n",
        "torch.save(TRG.vocab, model_dir + 'TRG_vocab.pt')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ66sAhMRY_V",
        "outputId": "53536431-33a6-4580-e18b-8f3ba4b5f811"
      },
      "source": [
        "model.load_state_dict(torch.load(model_dir + 'best-model.pt'))\n",
        "#model.load_state_dict(torch.load(model_dir + 'model-7.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.552 | Test PPL:  34.900 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_CPJEjnRd7O"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    #if isinstance(sentence, str):\n",
        "    #    nlp = spacy_en = spacy.load('en_core_web_sm')\n",
        "    #    tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
        "    #else:\n",
        "    #    tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "        \n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yhqTK4_RcmG",
        "outputId": "604e2570-3e7a-4708-968f-f2b2458666e2"
      },
      "source": [
        "example_idx =520\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['src']\n",
        "trg = vars(train_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['周', '三', '你', '可', '以', '载', '我', '到', '办', '公', '室', '吗']\n",
            "trg = ['can', 'you', 'give', 'me', 'a', 'ride', 'to', 'the', 'office', 'on', 'wednesday', '?']\n",
            "predicted trg = ['could', 'you', 'get', 'me', 'me', 'the', 'the', 'office', '?', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqaMUESBR_zk"
      },
      "source": [
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        \n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "        \n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbmhQXkPSEnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580e06bf-c4bd-452a-c923-8dd47e6f3de2"
      },
      "source": [
        "bleu_score = calculate_bleu(test_dataset, SRC, TRG, model, device)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score = 17.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5pgQYaDSGlA"
      },
      "source": [
        "def display_attention(sentence, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    #fontdict = {\"fontproperties\": zhfont}\n",
        "    \n",
        "    #ax.set_xticks(range(max(max_len_tar, len(predicted_seq))))\n",
        "    #ax.set_xlim(-0.5, max_len_tar -1.5)\n",
        "    \n",
        "    #ax.set_yticks(range(len(sentence) + 2))\n",
        "    #ax.set_xticklabels([subword_encoder_zh.decode([i]) for i in predicted_seq \n",
        "    #                    if i < subword_encoder_zh.vocab_size], \n",
        "    #                   fontdict=fontdict, fontsize=18)\n",
        "    \n",
        "    #plt.rcParams[\"font.family\"]=\"sans-serif\"\n",
        "    #plt.rcParams['font.sans-serif']=['STSong'] #用来正常显示中文标签\n",
        "    \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                       rotation=45 , fontproperties=myfont) #, fontdict=fontdict)\n",
        "    ax.set_yticklabels(['']+translation, fontproperties=myfont) # , fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvpxnXugriz0",
        "outputId": "b79153d3-805f-4a65-8795-fbb0e2c46bab"
      },
      "source": [
        "# 請在這邊自行調整 sample index \n",
        "# 觀察不同句子的 ATTENTION 結果\n",
        "example_idx =70\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['src']\n",
        "trg = vars(train_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['我', '每', '晚', '洗', '澡']\n",
            "trg = ['i', 'have', 'a', 'shower', 'every', 'night', '.']\n",
            "predicted trg = ['i', 'have', 'a', 'shower', 'every', 'night', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "keUMxSdSroWI",
        "outputId": "4bbfabde-81a9-4dfe-b92e-52d23672173f"
      },
      "source": [
        "print (\"\".join(src ))\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "我每晚洗澡\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8roJfGY-3bw"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}