{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhOs0IrWW1-E"
      },
      "source": [
        "## NMT English to Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_RByKlJWo1I"
      },
      "source": [
        "## 載入torch 0.6.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLryd8VynrYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4a4b9c-6e06-4496-afa0-95de958cdb2c"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 21.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.8.0+cu101)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.9.0\n",
            "    Uninstalling torchtext-0.9.0:\n",
            "      Successfully uninstalled torchtext-0.9.0\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc6fyby3WxIb"
      },
      "source": [
        "## 匯入package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5pj0lwocq8R"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import spacy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J78VW_MxdI7Q",
        "outputId": "6e2cbdf0-df18-4cba-9d90-eb222a317518"
      },
      "source": [
        "!pip install matplotlib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0VkC2DPcvlv",
        "outputId": "86725087-a05a-4131-bb42-c97a3a21fba7"
      },
      "source": [
        "\n",
        "# Colab 進行matplotlib繪圖時顯示繁體中文\n",
        "# 下載字體並命名taipei_sans_tc_beta.ttf，移至指定路徑\n",
        "!wget -O taipei_sans_tc_beta.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
        "!mv taipei_sans_tc_beta.ttf /usr/local/lib/python3.7/dist-packages/matplotlib//mpl-data/fonts/ttf\n",
        "\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import matplotlib.pyplot as plt \n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "import matplotlib.ticker as ticker\n",
        "# 自定義字體變數\n",
        "myfont = FontProperties(fname=r'/usr/local/lib/python3.7/dist-packages/matplotlib/mpl-data/fonts/ttf/taipei_sans_tc_beta.ttf')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-24 15:28:31--  https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.137.139, 74.125.137.100, 74.125.137.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.137.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/huib3mjv9bh3j09dqcfs92jaa9h9o0or/1616599650000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_ [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-03-24 15:28:33--  https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/huib3mjv9bh3j09dqcfs92jaa9h9o0or/1616599650000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)... 172.217.3.33, 2607:f8b0:4026:801::2001\n",
            "Connecting to doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)|172.217.3.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-font-ttf]\n",
            "Saving to: ‘taipei_sans_tc_beta.ttf’\n",
            "\n",
            "taipei_sans_tc_beta     [      <=>           ]  19.70M  15.2MB/s    in 1.3s    \n",
            "\n",
            "2021-03-24 15:28:36 (15.2 MB/s) - ‘taipei_sans_tc_beta.ttf’ saved [20659344]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIF1UdY_dm-g",
        "outputId": "ebec42d6-c6da-492b-d2dc-863ee0f6d9e0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96vuG4eddqNq",
        "outputId": "4f32d1ea-b535-413e-aef0-2e1672e1dee9"
      },
      "source": [
        "data_dir = '/content/drive/My Drive/data/'\n",
        "lines = open(data_dir + 'cmn.txt' , encoding='utf-8').read().strip().split('\\n')\n",
        "trnslt_pairs = [[s for s in l.split('\\t')] for l in lines ]\n",
        "print (\"Sample: \" , trnslt_pairs[1000][:2] )\n",
        "print (\"Total records:\" , len(trnslt_pairs))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample:  ['He was drowned.', '他被淹死了。']\n",
            "Total records: 24360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCQhVzxBeb4L",
        "outputId": "cdcfa17e-b435-4dcd-8dbb-77e8c1a8cc6a"
      },
      "source": [
        "# create train and validation set\n",
        "\n",
        "print (\"Total records after filtering :\" , len(trnslt_pairs))\n",
        "train, test = train_test_split(trnslt_pairs, test_size=0.1)\n",
        "train, val = train_test_split(train, test_size=0.1)\n",
        "print (\"training data:{} , develop data: {} , testing data: {}\".format(len(train),len(val),len(test)))\n",
        "    \n",
        "def write_csv(trn_data, file_path ):\n",
        "    with open(file_path ,'w', newline='', encoding='utf-8') as fout:\n",
        "        writer = csv.writer (fout)\n",
        "        for itm in trn_data: \n",
        "            writer.writerow ([itm[0],itm[1]])\n",
        "            \n",
        "file_path = data_dir + 'train.csv'\n",
        "write_csv(train, file_path )\n",
        "\n",
        "file_path = data_dir + 'val.csv'\n",
        "write_csv(val, file_path )\n",
        "    \n",
        "file_path = data_dir + 'test.csv'\n",
        "write_csv(test, file_path )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total records after filtering : 24360\n",
            "training data:19731 , develop data: 2193 , testing data: 2436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCWkf8che_x6"
      },
      "source": [
        "# 下載 spacy 的英文模型 幫我們做tokenize\n",
        "spacy_eng = spacy.load('en_core_web_sm')\n",
        "def tokenize_eng(text):\n",
        "  #清除不需要的字符\n",
        "  text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "  return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "def tokenize_cmn(text):\n",
        "  #去掉非中文字元\n",
        "  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
        "  text = regex.sub(' ', text)\n",
        "\n",
        "  return [word for word in text if word.strip()]\n",
        "\n",
        "\n",
        "TRG = Field(tokenize = tokenize_eng, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "SRC = Field(tokenize = tokenize_cmn, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            include_lengths = True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHZS3UvifctB",
        "outputId": "13403d25-4ad4-424a-eb9b-85dced68c395"
      },
      "source": [
        "# 讀取csv用TabularDataset\n",
        "train_dataset, dev_dataset, test_dataset = TabularDataset.splits(\n",
        "    path = data_dir , format = 'csv', skip_header = True,\n",
        "    train='train.csv', validation='val.csv', test='test.csv',\n",
        "    fields=[\n",
        "        ('trg', TRG),\n",
        "        ('src', SRC)\n",
        "    ]\n",
        ")\n",
        "SRC.build_vocab(train_dataset, min_freq = 1)\n",
        "TRG.build_vocab(train_dataset, min_freq = 1)\n",
        "\n",
        "print (\"中文語料的字元表長度: \" , len(SRC.vocab) , \", 英文的字元表長度: \" ,len(TRG.vocab))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "中文語料的字元表長度:  3349 , 英文的字元表長度:  6093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLgojEXCflOa"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dataset, dev_dataset, test_dataset), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.src),\n",
        "     device = device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oqp6B8VXfmB"
      },
      "source": [
        "* encoder_outputs 就是input每個hidden的輸出 假設有此句有4個詞，hidden dim為32 batch_size 為 2 那就是[4, 2, 32*2(bidirect\n",
        ")]\n",
        "* hidden 因為是要餵給decoder，所以用的是decoder的大小，此題是encode_hidden的2倍[2, 64]\n",
        "* mask 之後補"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VZFvX2mOLz_"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    # hidden bz , dec_hid_dim\n",
        "    # encoder_outputs src len, bz , enc_hid_dim x 2\n",
        "    # mask bz , src len\n",
        "    \n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "\n",
        "    hidden = hidden.unsqueeze(1) \n",
        "    # hidden unsqueeze bz , 1 , dec_hid_dim\n",
        "\n",
        "    attention = torch.matmul( hidden , encoder_outputs.permute(1, 2, 0))\n",
        "    # attention bz, 1 , src len\n",
        "    \n",
        "    attention = attention.squeeze(1)\n",
        "    # squeeze bz , src len\n",
        "\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "    return F.softmax(attention, dim = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NfXO9RzdNa7"
      },
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        input_dim: 3349\n",
        "        emb_dim: 300\n",
        "        enc_hid_dim: 256\n",
        "        dec_hid_dim: 512\n",
        "        dropout: 0.5\n",
        "        \"\"\"\n",
        "        # 我們的例子:[3349, 300]\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        # 雙向 ＧＲＵ  [300, 1, 256*2]\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        # [512, 512]\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        \"\"\"\n",
        "        src: [src len, batch size]  包起來的樣子[每一個batch的max_src_len, batch_szie]\n",
        "        src_len: [batch size]\n",
        "        \"\"\"\n",
        "\n",
        "        # [src len, batch size, emb dim]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "                \n",
        "        # 使用pack_padded_sequence 來壓縮序列        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "\n",
        "        # 使用 pad_packed_sequence 用來展開序列成原本形狀的      \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "            \n",
        "        #outputs shape [src len, batch size, hid dim * num directions]\n",
        "        #hidden shape [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden 堆疊 [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs 是最後一層 \n",
        "        \n",
        "        #hidden [-2, :, : ] 是最後一層 forwards RNN \n",
        "        #hidden [-1, :, : ] 是最後一層 backwards RNN\n",
        "        \n",
        "        # hidden 是最後再過一層 dense layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shm4X6vrlWG2"
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        # 單向 ＧＲＵ decoder \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        # 512 + 512 + 256\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "             \n",
        "        #input shape [batch size]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        #encoder_outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #mask shape [batch size, src len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input shape [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded shape [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "                \n",
        "        #a shape [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a shape [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs shape [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted shape [batch size, 1, enc hid dim * 2]\n",
        "        # a * value\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted shape [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input shape [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output shape [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden shape [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output shape [1, batch size, dec hid dim]\n",
        "        #hidden shape [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction shape [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7wrBbAJie6C"
      },
      "source": [
        "class Seq2SeqATTN(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "                    \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        #mask = [batch size, src len]\n",
        "                \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
        "            #  and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmxSwtLs8zBq",
        "outputId": "b8acde3d-064d-4dad-801b-05fd1e973a65"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 256 # 注意 encoder hidden layer 設定 必須為 dec 的一半 \n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2SeqATTN(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "\n",
        "def initial_mdl_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(initial_mdl_weights)\n",
        "print (\"模型全部參數量: {:10,d} \".format(sum(p.numel() for p in model.parameters())))\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "模型全部參數量: 13,330,946 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqATTN(\n",
              "  (encoder): RNNEncoder(\n",
              "    (embedding): Embedding(3372, 256)\n",
              "    (rnn): GRU(256, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): RNNDecoder(\n",
              "    (attention): Attention()\n",
              "    (embedding): Embedding(6146, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=6146, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNJnW0Sh86uH"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src, src_len = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, src_len.cpu() , trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, src_len.cpu(), trg, 0) #turn off teacher forcing\n",
        "            \n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "bLxQagjM9tc9",
        "outputId": "af4636b4-3ecc-41e1-daae-a744bcdb9ce3"
      },
      "source": [
        "MAX_EPOCHS = 30\n",
        "CLIP = 2\n",
        "model_dir =  '/content/drive/My Drive/data/'\n",
        "best_valid_loss = 9999999\n",
        "\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    torch.save(model.state_dict(), model_dir + 'model-{}.pt'.format(epoch))\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), model_dir + 'best-model.pt')\n",
        "   \n",
        "    print (\"Epoch {} training time: {:.2f} sec Training Loss: {:.3f} , Valiation Loss: {:.3f}\".format( epoch , end_time - start_time , train_loss , valid_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 training time: 483.81 sec Training Loss: 5.450 , Valiation Loss: 5.013\n",
            "Epoch 1 training time: 483.13 sec Training Loss: 4.496 , Valiation Loss: 4.670\n",
            "Epoch 2 training time: 479.65 sec Training Loss: 4.068 , Valiation Loss: 4.495\n",
            "Epoch 3 training time: 476.80 sec Training Loss: 3.714 , Valiation Loss: 4.251\n",
            "Epoch 4 training time: 474.63 sec Training Loss: 3.401 , Valiation Loss: 4.112\n",
            "Epoch 5 training time: 479.53 sec Training Loss: 3.083 , Valiation Loss: 3.936\n",
            "Epoch 6 training time: 482.56 sec Training Loss: 2.804 , Valiation Loss: 3.848\n",
            "Epoch 7 training time: 478.16 sec Training Loss: 2.557 , Valiation Loss: 3.749\n",
            "Epoch 8 training time: 480.75 sec Training Loss: 2.314 , Valiation Loss: 3.697\n",
            "Epoch 9 training time: 473.89 sec Training Loss: 2.132 , Valiation Loss: 3.682\n",
            "Epoch 10 training time: 471.48 sec Training Loss: 1.946 , Valiation Loss: 3.658\n",
            "Epoch 11 training time: 477.10 sec Training Loss: 1.781 , Valiation Loss: 3.632\n",
            "Epoch 12 training time: 472.65 sec Training Loss: 1.660 , Valiation Loss: 3.615\n",
            "Epoch 13 training time: 475.41 sec Training Loss: 1.551 , Valiation Loss: 3.651\n",
            "Epoch 14 training time: 473.42 sec Training Loss: 1.458 , Valiation Loss: 3.662\n",
            "Epoch 15 training time: 472.39 sec Training Loss: 1.371 , Valiation Loss: 3.653\n",
            "Epoch 16 training time: 476.77 sec Training Loss: 1.250 , Valiation Loss: 3.715\n",
            "Epoch 17 training time: 478.45 sec Training Loss: 1.201 , Valiation Loss: 3.729\n",
            "Epoch 18 training time: 475.65 sec Training Loss: 1.137 , Valiation Loss: 3.715\n",
            "Epoch 19 training time: 479.66 sec Training Loss: 1.059 , Valiation Loss: 3.753\n",
            "Epoch 20 training time: 479.99 sec Training Loss: 1.011 , Valiation Loss: 3.827\n",
            "Epoch 21 training time: 479.71 sec Training Loss: 0.948 , Valiation Loss: 3.835\n",
            "Epoch 22 training time: 481.02 sec Training Loss: 0.911 , Valiation Loss: 3.837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-215210fc8308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-daadda394409>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#trg = [trg len, batch size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-6b708e268076>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_len, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m#  and mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#receive output tensor (predictions) and new hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m#place predictions in a tensor holding predictions for each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-a14dacc4823a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#prediction shape [batch size, output dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqtVAjSE-5hb"
      },
      "source": [
        "# Save SRC and TRG vocab\n",
        "torch.save(SRC.vocab, model_dir + 'SRC_vocab.pt')\n",
        "torch.save(TRG.vocab, model_dir + 'TRG_vocab.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ66sAhMRY_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8af528d-6cd0-4d0f-9380-caa4fb6681ea"
      },
      "source": [
        "model.load_state_dict(torch.load(model_dir + 'best-model.pt'))\n",
        "#model.load_state_dict(torch.load(model_dir + 'model-7.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.729 | Test PPL:  41.638 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_CPJEjnRd7O"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    #if isinstance(sentence, str):\n",
        "    #    nlp = spacy_en = spacy.load('en_core_web_sm')\n",
        "    #    tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
        "    #else:\n",
        "    #    tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "        \n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yhqTK4_RcmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d989c423-6623-4bc5-b8d8-9ce4a296b6b6"
      },
      "source": [
        "example_idx =520\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['src']\n",
        "trg = vars(train_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['她', '企', '圖', '自', '殺']\n",
            "trg = ['she', 'tried', 'to', 'kill', 'herself', '.']\n",
            "predicted trg = ['she', 'tried', 'to', 'kill', 'herself', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqaMUESBR_zk"
      },
      "source": [
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        \n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "        \n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbmhQXkPSEnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb35511-c1fd-495b-a920-38965f39a5c2"
      },
      "source": [
        "bleu_score = calculate_bleu(test_dataset, SRC, TRG, model, device)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score = 17.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5pgQYaDSGlA"
      },
      "source": [
        "def display_attention(sentence, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    #fontdict = {\"fontproperties\": zhfont}\n",
        "    \n",
        "    #ax.set_xticks(range(max(max_len_tar, len(predicted_seq))))\n",
        "    #ax.set_xlim(-0.5, max_len_tar -1.5)\n",
        "    \n",
        "    #ax.set_yticks(range(len(sentence) + 2))\n",
        "    #ax.set_xticklabels([subword_encoder_zh.decode([i]) for i in predicted_seq \n",
        "    #                    if i < subword_encoder_zh.vocab_size], \n",
        "    #                   fontdict=fontdict, fontsize=18)\n",
        "    \n",
        "    #plt.rcParams[\"font.family\"]=\"sans-serif\"\n",
        "    #plt.rcParams['font.sans-serif']=['STSong'] #用来正常显示中文标签\n",
        "    \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                       rotation=45 , fontproperties=myfont) #, fontdict=fontdict)\n",
        "    ax.set_yticklabels(['']+translation, fontproperties=myfont) # , fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvpxnXugriz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f53542-34d5-4b02-8163-d6f9888d8574"
      },
      "source": [
        "# 請在這邊自行調整 sample index \n",
        "# 觀察不同句子的 ATTENTION 結果\n",
        "example_idx =70\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['src']\n",
        "trg = vars(train_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['能', '让', '我', '用', '这', '个', '吗']\n",
            "trg = ['am', 'i', 'allowed', 'to', 'use', 'this', '?']\n",
            "predicted trg = ['can', 'i', 'have', 'this', 'this', '?', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keUMxSdSroWI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "081fd345-da23-4be9-8776-d45890e837d4"
      },
      "source": [
        "print (\"\".join(src ))\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "能让我用这个吗\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8roJfGY-3bw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}