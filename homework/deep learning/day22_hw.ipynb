{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "for class.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osDYMvCTMRNg",
        "outputId": "265558e8-c431-4704-9438-bdda162bbca3"
      },
      "source": [
        "!pip uninstall spacy -y\n",
        "!pip install -U spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling spacy-2.2.4:\n",
            "  Successfully uninstalled spacy-2.2.4\n",
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/70/a0b8bd0cb54d8739ba4d6fb3458785c3b9b812b7fbe93b0f10beb1a53ada/spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 237kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.1.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/54/76982427ceb495dd19ff982c966708c624b85e03c45bf1912feaf60c7b2d/srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 52.4MB/s \n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/65/d5/6c58fc97f3098775e46d8202bf248752e626a8096a0ae9d76aa7c485a09c/spacy_legacy-3.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.2)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/5c/493a2f3bb0eac17b1d48129ecfd251f0520b6c89493e9fd0522f534a9e4a/catalogue-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (3.4.1)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 52.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=70b97437ecd41482f85c7505d28be524f81bd098abcd3182cdd083d7b725a8ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: pydantic, catalogue, srsly, typer, spacy-legacy, thinc, smart-open, pathy, spacy\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: smart-open 4.2.0\n",
            "    Uninstalling smart-open-4.2.0:\n",
            "      Successfully uninstalled smart-open-4.2.0\n",
            "Successfully installed catalogue-2.0.1 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.1 srsly-2.4.0 thinc-8.0.2 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abNPiYi_Mhlb",
        "outputId": "4c7a3ecf-52af-4310-8e27-6e943c1cc10d"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 11.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.8.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.9.0\n",
            "    Uninstalling torchtext-0.9.0:\n",
            "      Successfully uninstalled torchtext-0.9.0\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQx85R_Wb12r"
      },
      "source": [
        "import jieba\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "# from torchtext.data.metrics import bleu_score\n",
        "from pprint import pprint\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-TIO5Dvc0xg",
        "outputId": "1b655029-3700-4c74-9659-0b9b471d7abb"
      },
      "source": [
        "!mkdir ./data\n",
        "!mkdir ./data/multi30k\n",
        "!python -m spacy download en\n",
        "!ls ./data/multi30k -al\n",
        "spacy_english = spacy.load(\"en_core_web_sm\")\n",
        "!ls ./data/multi30k -al"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-24 15:11:48.530810: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 243kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Mar 24 15:11 .\n",
            "drwxr-xr-x 3 root root 4096 Mar 24 15:11 ..\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Mar 24 15:11 .\n",
            "drwxr-xr-x 3 root root 4096 Mar 24 15:11 ..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF982HzgdvIB",
        "outputId": "82344c89-aea8-417c-8476-f7d852c68f92"
      },
      "source": [
        "!python -m spacy download de\n",
        "spacy_de = spacy.load(\"de_core_news_sm\")\n",
        "!ls ./data/multi30k -al"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-24 15:11:57.117440: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "Collecting de-core-news-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.0.0/de_core_news_sm-3.0.0-py3-none-any.whl (19.3MB)\n",
            "\u001b[K     |████████████████████████████████| 19.3MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (54.1.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.0)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Mar 24 15:11 .\n",
            "drwxr-xr-x 3 root root 4096 Mar 24 15:11 ..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAE8YgKkeCeF"
      },
      "source": [
        "def tokenize_de(text):\n",
        "  return [token.text for token in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_english(text):\n",
        "  return [token.text for token in spacy_english.tokenizer(text)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47UrfAOqM991"
      },
      "source": [
        "sample run\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llk7ZMMIePkk",
        "outputId": "ab279f87-640f-4f2c-e331-c64ad262ce68"
      },
      "source": [
        "sample_text = \"I love machine learning\"\n",
        "print(tokenize_english(sample_text))\n",
        "\n",
        "german = Field(tokenize=tokenize_de, lower=True,\n",
        "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "english = Field(tokenize=tokenize_english, lower=True,\n",
        "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = (\".en\", \".de\"),\n",
        "                                                    fields=(english, german))\n",
        "\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "\n",
        "print(f\"Unique tokens in source (en) vocabulary: {len(english.vocab)}\")\n",
        "print(f\"Unique tokens in target (german) vocabulary: {len(german.vocab)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'love', 'machine', 'learning']\n",
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 597kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 170kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 166kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (en) vocabulary: 5893\n",
            "Unique tokens in target (german) vocabulary: 7853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf1ZLlFbelMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c28ff4-5528-46cc-a150-3b2f2c2a2c7a"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
        "\n",
        "print(train_data[5].__dict__.keys())\n",
        "pprint(train_data[5].__dict__.values())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n",
            "dict_keys(['src', 'trg'])\n",
            "dict_values([['a', 'man', 'in', 'green', 'holds', 'a', 'guitar', 'while', 'the', 'other', 'man', 'observes', 'his', 'shirt', '.'], ['ein', 'mann', 'in', 'grün', 'hält', 'eine', 'gitarre', ',', 'während', 'der', 'andere', 'mann', 'sein', 'hemd', 'ansieht', '.']])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7OPLPTxNkvr"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.src),\n",
        "    device = device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2c55GJPP23L",
        "outputId": "73cc038d-cbac-487c-b41e-328d5352eae5"
      },
      "source": [
        "count = 0\n",
        "max_len_eng = []\n",
        "max_len_ger = []\n",
        "for data in train_data:\n",
        "  max_len_eng.append(len(data.src))\n",
        "  max_len_ger.append(len(data.trg))\n",
        "  if count < 10 :\n",
        "    print(\"English - \", *data.src, \" Length - \", len(data.src))\n",
        "    print(\"German - \", *data.trg, \" Length - \", len(data.trg))\n",
        "    print()\n",
        "  count += 1\n",
        "\n",
        "print(\"Maximum Length of English sentence {} and German sentence {} in the dataset\".format(max(max_len_eng),max(max_len_ger)))\n",
        "print(\"Minimum Length of English sentence {} and German sentence {} in the dataset\".format(min(max_len_eng),min(max_len_ger)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English -  two young , white males are outside near many bushes .  Length -  11\n",
            "German -  zwei junge weiße männer sind im freien in der nähe vieler büsche .  Length -  13\n",
            "\n",
            "English -  several men in hard hats are operating a giant pulley system .  Length -  12\n",
            "German -  mehrere männer mit schutzhelmen bedienen ein antriebsradsystem .  Length -  8\n",
            "\n",
            "English -  a little girl climbing into a wooden playhouse .  Length -  9\n",
            "German -  ein kleines mädchen klettert in ein spielhaus aus holz .  Length -  10\n",
            "\n",
            "English -  a man in a blue shirt is standing on a ladder cleaning a window .  Length -  15\n",
            "German -  ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster .  Length -  15\n",
            "\n",
            "English -  two men are at the stove preparing food .  Length -  9\n",
            "German -  zwei männer stehen am herd und bereiten essen zu .  Length -  10\n",
            "\n",
            "English -  a man in green holds a guitar while the other man observes his shirt .  Length -  15\n",
            "German -  ein mann in grün hält eine gitarre , während der andere mann sein hemd ansieht .  Length -  16\n",
            "\n",
            "English -  a man is smiling at a stuffed lion  Length -  8\n",
            "German -  ein mann lächelt einen ausgestopften löwen an .  Length -  8\n",
            "\n",
            "English -  a trendy girl talking on her cellphone while gliding slowly down the street .  Length -  14\n",
            "German -  ein schickes mädchen spricht mit dem handy während sie langsam die straße entlangschwebt .  Length -  14\n",
            "\n",
            "English -  a woman with a large purse is walking by a gate .  Length -  12\n",
            "German -  eine frau mit einer großen geldbörse geht an einem tor vorbei .  Length -  12\n",
            "\n",
            "English -  boys dancing on poles in the middle of the night .  Length -  11\n",
            "German -  jungen tanzen mitten in der nacht auf pfosten .  Length -  9\n",
            "\n",
            "Maximum Length of English sentence 41 and German sentence 44 in the dataset\n",
            "Minimum Length of English sentence 4 and German sentence 1 in the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r_Dd2rITf-G"
      },
      "source": [
        "iterator dim = [max_src_len, batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvMhQN0vP_fQ",
        "outputId": "fb251750-fbb1-4875-867b-93ef19562456"
      },
      "source": [
        "count = 0\n",
        "for data in train_iterator:\n",
        "  if count < 1 :\n",
        "    print(\"Shapes\", data.src.shape, data.trg.shape)\n",
        "    print()\n",
        "    print(\"English - \",*data.src, \" Length - \", len(data.src))\n",
        "    print()\n",
        "    print(\"German - \",*data.trg, \" Length - \", len(data.trg))\n",
        "    temp_eng = data.src\n",
        "    temp_ger = data.trg\n",
        "    count += 1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shapes torch.Size([12, 32]) torch.Size([18, 32])\n",
            "\n",
            "English -  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([   4,    4,  697,   46,  342,    7,    4,    4, 2322,   16,   30,    4,\n",
            "         176,    4,    4,    4,    4,   16,    4,    7,  322,    4,    4,    4,\n",
            "           4,    4,   48,    4,    4,   64,    4,    4], device='cuda:0') tensor([  24,   14,    6, 1094,   12,    9,   14,   38,   17,   63,    6,   33,\n",
            "          17,   24,  295,    9,  267, 1397, 5610,   30,   15,   24,   64,    9,\n",
            "          38,   14,   19,    9,    9,  169,    9,    9], device='cuda:0') tensor([  33,    6, 1077, 1846,    4,  241,    6,   12,  399,   22,   31,    6,\n",
            "         202,   33,  467,    6,   39,   30,  962,   17,  322,   33,    6,   73,\n",
            "          12,   22,   17,   10,    6,    4,  140,   22], device='cuda:0') tensor([  10,  204,   32,  601,   39,   27,   52,   19,   66,  175, 4695,    4,\n",
            "          19,   22,   89,  146,   13,   17,    8,  195, 3920,   22,    4,  340,\n",
            "         938,   26,   37,  230,    4,  319,    4,    4], device='cuda:0') tensor([ 917, 2030,  775,  336,    6,  157, 1391,  100,  494,    0,   17,   31,\n",
            "          57,    4,    8,  107, 1822,   78,   44,    0,  722,    4, 1070, 2766,\n",
            "         152,   11,    6,   51,  231,   99,  153,   29], device='cuda:0') tensor([   4,  212,   13,   82, 3339,   51,   60,   57,    8,   47,  269,   23,\n",
            "          12,  590,  109,   20,  292,    6, 2771,   18, 1567,  430,   10, 4565,\n",
            "          40,   25,   95,    4,   10,  838,  189,   23], device='cuda:0') tensor([ 715,  171,   19,   57, 2205,    7,    4,   20,    7,   20,    6,   10,\n",
            "           4,    6,   12,   27,   66,    4,    6,  663,  544,   67,    8,  102,\n",
            "           4,  483,  222,  262,  483,    6,   10,   10], device='cuda:0') tensor([2854,    4,    6,    7,   12,  291,   39,  305,   52,  167,    4,   57,\n",
            "          59,   21,   27, 1548,  710, 1913,    7,    7,    7,   10,    4, 1800,\n",
            "         101,    4,  101,   59,    4,    4,   45,   37], device='cuda:0') tensor([  57,   39,  725,  941, 1604,  108,  988,  119,   85,   72, 1364,  883,\n",
            "          77, 5158,   99,  953,  545,  180,  704, 3678,  809,  419,   84, 2255,\n",
            "          39,  142,  622,  166,   39,    0, 3040,  534], device='cuda:0') tensor([  5,   5,   5,   5, 101,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
            "          5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
            "          5,   5,   5,   5], device='cuda:0') tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')  Length -  12\n",
            "\n",
            "German -  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([   5,    8,  773,    5,  148,   15,    8, 5888, 2338,   18,   30,    5,\n",
            "         416,    5,    5,    5,    8,   18,    5,   17, 2919,    5,    5,    5,\n",
            "           8,    8,   43,    5,    5,    8,    5,    5], device='cuda:0') tensor([  66,   16,   31,    0,   14,   13,   16,   36,  141,   65,   11,   25,\n",
            "          84,   25,  618,   13,    0, 2224,    0,   30, 2444,   25,  431,   13,\n",
            "          36,   16,   41,   13,   13,   67,   13,   13], device='cuda:0') tensor([  25,    7,   20,    9,   34,   62,    7,   22,   11,    7,   50,    7,\n",
            "         191,   11,   29,   11,   34,   30, 2010, 1024,   10,   11,   20,    9,\n",
            "        1005,    9,   57,  217,    7,   68,   61,    7], device='cuda:0') tensor([1412,  449,  409,   69,   20,   11,   82,   54,  127,  160, 1227,  237,\n",
            "          73,    6,  278,  138,   11,  137,   23, 2563,    0,    6, 1074,   15,\n",
            "          60,  173,    7,   12,  478,    8,    5,    6], device='cuda:0') tensor([  19, 1980,   10,   12, 4430,   77, 6443,   29,  705, 2408,  623,   79,\n",
            "         131,  697,   12,  270, 3689,    7,   11,   64,  482,    0,   12,  166,\n",
            "           8,   10,   33,   19,  539,    0,  164,   46], device='cuda:0') tensor([ 605,   83, 1209,   19,   14,   87,   62,  444,   12,   63,    7,   48,\n",
            "          59,    7,   77,  132,   12,    6, 2923,   33,  306,   12,    6, 5558,\n",
            "          34,  232,  356,  227,    8,    7,    9,   40], device='cuda:0') tensor([3428,   42,    9,   94,  152,   17,   58,  131,   14, 2477,    6,  131,\n",
            "           6,    6,  259, 5924,   24, 3352,   27,  122,   33,   24,  495,   56,\n",
            "         154, 1788,  650,   78,   34,    6,   39,   38], device='cuda:0') tensor([  20,    8,   20,  497,    7, 2046,    5,   10,   82,    4, 1530,   10,\n",
            "          78,  984,    4, 1098, 6836,    4,   24,   28,  366,  143,    4,   24,\n",
            "           4,    9,   14,  214,    4, 1199,    8,  596], device='cuda:0') tensor([  88,   34,  670,  151,   33,  119, 2416,  750,  247,    3,    4, 1160,\n",
            "         104,    4,    3,    4,    4,    3,  763,    0, 1314,  411,    3, 3214,\n",
            "           3,  539,  754,    4,    3,    4,  138,    4], device='cuda:0') tensor([   4,    4,   84, 4995, 2438,    4,    4,    4,   64,    1,    3,    4,\n",
            "           4,    3,    1,    3,    3,    1,    4,    4,    3,    4,    1,   15,\n",
            "           1,    8,    4,    3,    1,    3,    7,    3], device='cuda:0') tensor([ 3,  3, 54,  4,  4,  3,  3,  3, 17,  1,  1,  3,  3,  1,  1,  1,  1,  1,\n",
            "         3,  3,  1,  3,  1,  0,  1, 34,  3,  1,  1,  1, 15,  1],\n",
            "       device='cuda:0') tensor([   1,    1,    4,    3,    3,    1,    1,    1, 1898,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    0,\n",
            "           1,    4,    1,    1,    1,    1,   81,    1], device='cuda:0') tensor([   1,    1,    3,    1,    1,    1,    1,    1,    4,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1, 3282,\n",
            "           1,    3,    1,    1,    1,    1,   37,    1], device='cuda:0') tensor([  1,   1,   1,   1,   1,   1,   1,   1,   3,   1,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1,   1,   1,   1,   1,   1, 358,   1,   1,   1,   1,\n",
            "          1,   1,   4,   1], device='cuda:0') tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1, 48,  1,  1,  1,  1,  1,  1,  3,  1],\n",
            "       device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')  Length -  18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMaKs9__UOjI"
      },
      "source": [
        "取得numpy型式的值\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dup4nlwQ7mc"
      },
      "source": [
        "temp_eng_idx = (temp_eng).cpu().detach().numpy()\n",
        "temp_ger_idx = (temp_ger).cpu().detach().numpy()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "7SkZMnKtUHJa",
        "outputId": "5994668f-d4bb-439e-bd45-8313f5c8efa3"
      },
      "source": [
        "df_eng_idx = pd.DataFrame(\n",
        "    data = temp_eng_idx, columns = [str(\"S_\")+str(x) for x in range(1, 33)])\n",
        "df_eng_idx.index.name = 'Time Steps'\n",
        "df_eng_idx.index = df_eng_idx.index + 1 \n",
        "# df_eng_idx.to_csv('/content/idx.csv')\n",
        "df_eng_idx"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S_1</th>\n",
              "      <th>S_2</th>\n",
              "      <th>S_3</th>\n",
              "      <th>S_4</th>\n",
              "      <th>S_5</th>\n",
              "      <th>S_6</th>\n",
              "      <th>S_7</th>\n",
              "      <th>S_8</th>\n",
              "      <th>S_9</th>\n",
              "      <th>S_10</th>\n",
              "      <th>S_11</th>\n",
              "      <th>S_12</th>\n",
              "      <th>S_13</th>\n",
              "      <th>S_14</th>\n",
              "      <th>S_15</th>\n",
              "      <th>S_16</th>\n",
              "      <th>S_17</th>\n",
              "      <th>S_18</th>\n",
              "      <th>S_19</th>\n",
              "      <th>S_20</th>\n",
              "      <th>S_21</th>\n",
              "      <th>S_22</th>\n",
              "      <th>S_23</th>\n",
              "      <th>S_24</th>\n",
              "      <th>S_25</th>\n",
              "      <th>S_26</th>\n",
              "      <th>S_27</th>\n",
              "      <th>S_28</th>\n",
              "      <th>S_29</th>\n",
              "      <th>S_30</th>\n",
              "      <th>S_31</th>\n",
              "      <th>S_32</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time Steps</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>697</td>\n",
              "      <td>46</td>\n",
              "      <td>342</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2322</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>176</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>322</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>48</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>1094</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>38</td>\n",
              "      <td>17</td>\n",
              "      <td>63</td>\n",
              "      <td>6</td>\n",
              "      <td>33</td>\n",
              "      <td>17</td>\n",
              "      <td>24</td>\n",
              "      <td>295</td>\n",
              "      <td>9</td>\n",
              "      <td>267</td>\n",
              "      <td>1397</td>\n",
              "      <td>5610</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>24</td>\n",
              "      <td>64</td>\n",
              "      <td>9</td>\n",
              "      <td>38</td>\n",
              "      <td>14</td>\n",
              "      <td>19</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>169</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>1077</td>\n",
              "      <td>1846</td>\n",
              "      <td>4</td>\n",
              "      <td>241</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>399</td>\n",
              "      <td>22</td>\n",
              "      <td>31</td>\n",
              "      <td>6</td>\n",
              "      <td>202</td>\n",
              "      <td>33</td>\n",
              "      <td>467</td>\n",
              "      <td>6</td>\n",
              "      <td>39</td>\n",
              "      <td>30</td>\n",
              "      <td>962</td>\n",
              "      <td>17</td>\n",
              "      <td>322</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>73</td>\n",
              "      <td>12</td>\n",
              "      <td>22</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>140</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10</td>\n",
              "      <td>204</td>\n",
              "      <td>32</td>\n",
              "      <td>601</td>\n",
              "      <td>39</td>\n",
              "      <td>27</td>\n",
              "      <td>52</td>\n",
              "      <td>19</td>\n",
              "      <td>66</td>\n",
              "      <td>175</td>\n",
              "      <td>4695</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>22</td>\n",
              "      <td>89</td>\n",
              "      <td>146</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>8</td>\n",
              "      <td>195</td>\n",
              "      <td>3920</td>\n",
              "      <td>22</td>\n",
              "      <td>4</td>\n",
              "      <td>340</td>\n",
              "      <td>938</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>230</td>\n",
              "      <td>4</td>\n",
              "      <td>319</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>917</td>\n",
              "      <td>2030</td>\n",
              "      <td>775</td>\n",
              "      <td>336</td>\n",
              "      <td>6</td>\n",
              "      <td>157</td>\n",
              "      <td>1391</td>\n",
              "      <td>100</td>\n",
              "      <td>494</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>31</td>\n",
              "      <td>57</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>107</td>\n",
              "      <td>1822</td>\n",
              "      <td>78</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>722</td>\n",
              "      <td>4</td>\n",
              "      <td>1070</td>\n",
              "      <td>2766</td>\n",
              "      <td>152</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>51</td>\n",
              "      <td>231</td>\n",
              "      <td>99</td>\n",
              "      <td>153</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>212</td>\n",
              "      <td>13</td>\n",
              "      <td>82</td>\n",
              "      <td>3339</td>\n",
              "      <td>51</td>\n",
              "      <td>60</td>\n",
              "      <td>57</td>\n",
              "      <td>8</td>\n",
              "      <td>47</td>\n",
              "      <td>269</td>\n",
              "      <td>23</td>\n",
              "      <td>12</td>\n",
              "      <td>590</td>\n",
              "      <td>109</td>\n",
              "      <td>20</td>\n",
              "      <td>292</td>\n",
              "      <td>6</td>\n",
              "      <td>2771</td>\n",
              "      <td>18</td>\n",
              "      <td>1567</td>\n",
              "      <td>430</td>\n",
              "      <td>10</td>\n",
              "      <td>4565</td>\n",
              "      <td>40</td>\n",
              "      <td>25</td>\n",
              "      <td>95</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>838</td>\n",
              "      <td>189</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>715</td>\n",
              "      <td>171</td>\n",
              "      <td>19</td>\n",
              "      <td>57</td>\n",
              "      <td>2205</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>7</td>\n",
              "      <td>20</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>27</td>\n",
              "      <td>66</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>663</td>\n",
              "      <td>544</td>\n",
              "      <td>67</td>\n",
              "      <td>8</td>\n",
              "      <td>102</td>\n",
              "      <td>4</td>\n",
              "      <td>483</td>\n",
              "      <td>222</td>\n",
              "      <td>262</td>\n",
              "      <td>483</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2854</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>291</td>\n",
              "      <td>39</td>\n",
              "      <td>305</td>\n",
              "      <td>52</td>\n",
              "      <td>167</td>\n",
              "      <td>4</td>\n",
              "      <td>57</td>\n",
              "      <td>59</td>\n",
              "      <td>21</td>\n",
              "      <td>27</td>\n",
              "      <td>1548</td>\n",
              "      <td>710</td>\n",
              "      <td>1913</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>1800</td>\n",
              "      <td>101</td>\n",
              "      <td>4</td>\n",
              "      <td>101</td>\n",
              "      <td>59</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>57</td>\n",
              "      <td>39</td>\n",
              "      <td>725</td>\n",
              "      <td>941</td>\n",
              "      <td>1604</td>\n",
              "      <td>108</td>\n",
              "      <td>988</td>\n",
              "      <td>119</td>\n",
              "      <td>85</td>\n",
              "      <td>72</td>\n",
              "      <td>1364</td>\n",
              "      <td>883</td>\n",
              "      <td>77</td>\n",
              "      <td>5158</td>\n",
              "      <td>99</td>\n",
              "      <td>953</td>\n",
              "      <td>545</td>\n",
              "      <td>180</td>\n",
              "      <td>704</td>\n",
              "      <td>3678</td>\n",
              "      <td>809</td>\n",
              "      <td>419</td>\n",
              "      <td>84</td>\n",
              "      <td>2255</td>\n",
              "      <td>39</td>\n",
              "      <td>142</td>\n",
              "      <td>622</td>\n",
              "      <td>166</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>3040</td>\n",
              "      <td>534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>101</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             S_1   S_2   S_3   S_4   S_5  ...  S_28  S_29  S_30  S_31  S_32\n",
              "Time Steps                                ...                              \n",
              "1              2     2     2     2     2  ...     2     2     2     2     2\n",
              "2              4     4   697    46   342  ...     4     4    64     4     4\n",
              "3             24    14     6  1094    12  ...     9     9   169     9     9\n",
              "4             33     6  1077  1846     4  ...    10     6     4   140    22\n",
              "5             10   204    32   601    39  ...   230     4   319     4     4\n",
              "6            917  2030   775   336     6  ...    51   231    99   153    29\n",
              "7              4   212    13    82  3339  ...     4    10   838   189    23\n",
              "8            715   171    19    57  2205  ...   262   483     6    10    10\n",
              "9           2854     4     6     7    12  ...    59     4     4    45    37\n",
              "10            57    39   725   941  1604  ...   166    39     0  3040   534\n",
              "11             5     5     5     5   101  ...     5     5     5     5     5\n",
              "12             3     3     3     3     3  ...     3     3     3     3     3\n",
              "\n",
              "[12 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "_Hcek338UIBf",
        "outputId": "a1d14192-382c-42c7-d231-8740440750ff"
      },
      "source": [
        "idx_2_word = {}\n",
        "\n",
        "for idx, w in enumerate(english.vocab.itos):\n",
        "  idx_2_word[idx] = w\n",
        "\n",
        "df_eng_word = pd.DataFrame(columns = [str(\"S_\")+str(x) for x in range(1, 33)])\n",
        "df_eng_word = df_eng_idx.replace(idx_2_word)\n",
        "# df_eng_word.to_csv('/content/Words.csv')\n",
        "df_eng_word"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S_1</th>\n",
              "      <th>S_2</th>\n",
              "      <th>S_3</th>\n",
              "      <th>S_4</th>\n",
              "      <th>S_5</th>\n",
              "      <th>S_6</th>\n",
              "      <th>S_7</th>\n",
              "      <th>S_8</th>\n",
              "      <th>S_9</th>\n",
              "      <th>S_10</th>\n",
              "      <th>S_11</th>\n",
              "      <th>S_12</th>\n",
              "      <th>S_13</th>\n",
              "      <th>S_14</th>\n",
              "      <th>S_15</th>\n",
              "      <th>S_16</th>\n",
              "      <th>S_17</th>\n",
              "      <th>S_18</th>\n",
              "      <th>S_19</th>\n",
              "      <th>S_20</th>\n",
              "      <th>S_21</th>\n",
              "      <th>S_22</th>\n",
              "      <th>S_23</th>\n",
              "      <th>S_24</th>\n",
              "      <th>S_25</th>\n",
              "      <th>S_26</th>\n",
              "      <th>S_27</th>\n",
              "      <th>S_28</th>\n",
              "      <th>S_29</th>\n",
              "      <th>S_30</th>\n",
              "      <th>S_31</th>\n",
              "      <th>S_32</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time Steps</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "      <td>&lt;sos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>artist</td>\n",
              "      <td>one</td>\n",
              "      <td>photo</td>\n",
              "      <td>the</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>jockeys</td>\n",
              "      <td>two</td>\n",
              "      <td>men</td>\n",
              "      <td>a</td>\n",
              "      <td>there</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>two</td>\n",
              "      <td>a</td>\n",
              "      <td>the</td>\n",
              "      <td>run</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>three</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>person</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>young</td>\n",
              "      <td>woman</td>\n",
              "      <td>in</td>\n",
              "      <td>professional</td>\n",
              "      <td>of</td>\n",
              "      <td>man</td>\n",
              "      <td>woman</td>\n",
              "      <td>group</td>\n",
              "      <td>are</td>\n",
              "      <td>children</td>\n",
              "      <td>in</td>\n",
              "      <td>girl</td>\n",
              "      <td>are</td>\n",
              "      <td>young</td>\n",
              "      <td>motorcycle</td>\n",
              "      <td>man</td>\n",
              "      <td>busy</td>\n",
              "      <td>uniformed</td>\n",
              "      <td>showgirl</td>\n",
              "      <td>men</td>\n",
              "      <td>,</td>\n",
              "      <td>young</td>\n",
              "      <td>person</td>\n",
              "      <td>man</td>\n",
              "      <td>group</td>\n",
              "      <td>woman</td>\n",
              "      <td>people</td>\n",
              "      <td>man</td>\n",
              "      <td>man</td>\n",
              "      <td>doing</td>\n",
              "      <td>man</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>girl</td>\n",
              "      <td>in</td>\n",
              "      <td>courtyard</td>\n",
              "      <td>wrestler</td>\n",
              "      <td>a</td>\n",
              "      <td>rides</td>\n",
              "      <td>in</td>\n",
              "      <td>of</td>\n",
              "      <td>racing</td>\n",
              "      <td>wearing</td>\n",
              "      <td>red</td>\n",
              "      <td>in</td>\n",
              "      <td>many</td>\n",
              "      <td>girl</td>\n",
              "      <td>rider</td>\n",
              "      <td>in</td>\n",
              "      <td>street</td>\n",
              "      <td>men</td>\n",
              "      <td>puts</td>\n",
              "      <td>are</td>\n",
              "      <td>run</td>\n",
              "      <td>girl</td>\n",
              "      <td>in</td>\n",
              "      <td>dressed</td>\n",
              "      <td>of</td>\n",
              "      <td>wearing</td>\n",
              "      <td>are</td>\n",
              "      <td>is</td>\n",
              "      <td>in</td>\n",
              "      <td>a</td>\n",
              "      <td>holds</td>\n",
              "      <td>wearing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>is</td>\n",
              "      <td>purple</td>\n",
              "      <td>sitting</td>\n",
              "      <td>leaps</td>\n",
              "      <td>street</td>\n",
              "      <td>his</td>\n",
              "      <td>green</td>\n",
              "      <td>people</td>\n",
              "      <td>their</td>\n",
              "      <td>jeans</td>\n",
              "      <td>bowties</td>\n",
              "      <td>a</td>\n",
              "      <td>people</td>\n",
              "      <td>wearing</td>\n",
              "      <td>stands</td>\n",
              "      <td>glasses</td>\n",
              "      <td>with</td>\n",
              "      <td>are</td>\n",
              "      <td>on</td>\n",
              "      <td>using</td>\n",
              "      <td>careful</td>\n",
              "      <td>wearing</td>\n",
              "      <td>a</td>\n",
              "      <td>like</td>\n",
              "      <td>pedestrians</td>\n",
              "      <td>black</td>\n",
              "      <td>playing</td>\n",
              "      <td>climbing</td>\n",
              "      <td>a</td>\n",
              "      <td>trick</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>rolling</td>\n",
              "      <td>leggings</td>\n",
              "      <td>drawing</td>\n",
              "      <td>onto</td>\n",
              "      <td>in</td>\n",
              "      <td>bicycle</td>\n",
              "      <td>skates</td>\n",
              "      <td>stand</td>\n",
              "      <td>horses</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>are</td>\n",
              "      <td>red</td>\n",
              "      <td>outside</td>\n",
              "      <td>a</td>\n",
              "      <td>on</td>\n",
              "      <td>looks</td>\n",
              "      <td>shoppers</td>\n",
              "      <td>riding</td>\n",
              "      <td>her</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>do</td>\n",
              "      <td>a</td>\n",
              "      <td>wheelchair</td>\n",
              "      <td>late</td>\n",
              "      <td>walk</td>\n",
              "      <td>and</td>\n",
              "      <td>in</td>\n",
              "      <td>up</td>\n",
              "      <td>uniform</td>\n",
              "      <td>bike</td>\n",
              "      <td>baby</td>\n",
              "      <td>blue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>a</td>\n",
              "      <td>runs</td>\n",
              "      <td>with</td>\n",
              "      <td>another</td>\n",
              "      <td>chinatown</td>\n",
              "      <td>up</td>\n",
              "      <td>through</td>\n",
              "      <td>outside</td>\n",
              "      <td>on</td>\n",
              "      <td>water</td>\n",
              "      <td>singing</td>\n",
              "      <td>shirt</td>\n",
              "      <td>of</td>\n",
              "      <td>bikini</td>\n",
              "      <td>top</td>\n",
              "      <td>at</td>\n",
              "      <td>making</td>\n",
              "      <td>in</td>\n",
              "      <td>lipstick</td>\n",
              "      <td>to</td>\n",
              "      <td>n't</td>\n",
              "      <td>party</td>\n",
              "      <td>is</td>\n",
              "      <td>80</td>\n",
              "      <td>down</td>\n",
              "      <td>white</td>\n",
              "      <td>snow</td>\n",
              "      <td>a</td>\n",
              "      <td>is</td>\n",
              "      <td>move</td>\n",
              "      <td>who</td>\n",
              "      <td>shirt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>giant</td>\n",
              "      <td>across</td>\n",
              "      <td>people</td>\n",
              "      <td>outside</td>\n",
              "      <td>section</td>\n",
              "      <td>the</td>\n",
              "      <td>a</td>\n",
              "      <td>at</td>\n",
              "      <td>the</td>\n",
              "      <td>at</td>\n",
              "      <td>in</td>\n",
              "      <td>is</td>\n",
              "      <td>a</td>\n",
              "      <td>in</td>\n",
              "      <td>of</td>\n",
              "      <td>his</td>\n",
              "      <td>their</td>\n",
              "      <td>a</td>\n",
              "      <td>in</td>\n",
              "      <td>break</td>\n",
              "      <td>hit</td>\n",
              "      <td>hat</td>\n",
              "      <td>on</td>\n",
              "      <td>'s</td>\n",
              "      <td>a</td>\n",
              "      <td>crossing</td>\n",
              "      <td>covered</td>\n",
              "      <td>very</td>\n",
              "      <td>crossing</td>\n",
              "      <td>in</td>\n",
              "      <td>is</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>snowball</td>\n",
              "      <td>a</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "      <td>of</td>\n",
              "      <td>brick</td>\n",
              "      <td>street</td>\n",
              "      <td>night</td>\n",
              "      <td>green</td>\n",
              "      <td>each</td>\n",
              "      <td>a</td>\n",
              "      <td>outside</td>\n",
              "      <td>large</td>\n",
              "      <td>an</td>\n",
              "      <td>his</td>\n",
              "      <td>electronic</td>\n",
              "      <td>way</td>\n",
              "      <td>motorized</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>is</td>\n",
              "      <td>a</td>\n",
              "      <td>movie</td>\n",
              "      <td>city</td>\n",
              "      <td>a</td>\n",
              "      <td>city</td>\n",
              "      <td>large</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>holding</td>\n",
              "      <td>playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>outside</td>\n",
              "      <td>street</td>\n",
              "      <td>foreground</td>\n",
              "      <td>ring</td>\n",
              "      <td>us</td>\n",
              "      <td>wall</td>\n",
              "      <td>fair</td>\n",
              "      <td>talking</td>\n",
              "      <td>field</td>\n",
              "      <td>other</td>\n",
              "      <td>choir</td>\n",
              "      <td>crying</td>\n",
              "      <td>building</td>\n",
              "      <td>innertube</td>\n",
              "      <td>bike</td>\n",
              "      <td>device</td>\n",
              "      <td>home</td>\n",
              "      <td>boat</td>\n",
              "      <td>mirror</td>\n",
              "      <td>soil</td>\n",
              "      <td>van</td>\n",
              "      <td>laughing</td>\n",
              "      <td>sidewalk</td>\n",
              "      <td>batman</td>\n",
              "      <td>street</td>\n",
              "      <td>road</td>\n",
              "      <td>streets</td>\n",
              "      <td>rock</td>\n",
              "      <td>street</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>eyeglasses</td>\n",
              "      <td>drums</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>city</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 S_1       S_2         S_3  ...    S_30        S_31     S_32\n",
              "Time Steps                                  ...                             \n",
              "1              <sos>     <sos>       <sos>  ...   <sos>       <sos>    <sos>\n",
              "2                  a         a      artist  ...  person           a        a\n",
              "3              young     woman          in  ...   doing         man      man\n",
              "4               girl        in   courtyard  ...       a       holds  wearing\n",
              "5                 is    purple     sitting  ...   trick           a        a\n",
              "6            rolling  leggings     drawing  ...    bike        baby     blue\n",
              "7                  a      runs        with  ...    move         who    shirt\n",
              "8              giant    across      people  ...      in          is       is\n",
              "9           snowball         a          in  ...       a     holding  playing\n",
              "10           outside    street  foreground  ...   <unk>  eyeglasses    drums\n",
              "11                 .         .           .  ...       .           .        .\n",
              "12             <eos>     <eos>       <eos>  ...   <eos>       <eos>    <eos>\n",
              "\n",
              "[12 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAVTVM8-cN7B"
      },
      "source": [
        "* LSTM輸入格式需為[src_len, batch_size, feature_size](batch_first – 默認是False)\n",
        "* 以我們的例子，需帶入[src_len, 32, 300]進入LSTM\n",
        "* LSTM輸出有:\n",
        "  * outputs: [src_len, 32, num_directions * hidden_size]\n",
        "  * hidden_state: [num_layers * num_directions, 32, hidden_size]\n",
        "  * cell_state: [num_layers * num_directions, 32, hidden_size]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO6Lt359UtkD"
      },
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "    \"\"\"\n",
        "    input_size: encoder的輸入，one hot vector 的型式，也就是字典大小\n",
        "    embedding_size: embedding dim的大小，表示你希望把字濃縮成幾維的向量\n",
        "    hidden_size: 賦予lstm 隱藏層的神經元數量\n",
        "    num_layers: 想要幾層的lstm\n",
        "    p: dropout的%數\n",
        "    \"\"\"\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    # 我們的例子:[5893, 300]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    # [300, 2, 1024]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    x: 我們上一個cell包起來的樣子[每一個batch的max_src_len, batch_szie]\n",
        "    \"\"\"\n",
        "    # embedding: [26, 32, 300]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # outputs [26, 32, 1024] [max_src_len , batch_size , hidden_size]\n",
        "    # (hs, cs) [2, 32, 1024] , [2, 32, 1024] [num_layers, batch_size size, hidden_size]\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "    \n",
        "    return hidden_state, cell_state"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRCo-9I4d5Bp",
        "outputId": "078fe918-d786-49c8-b809-efd636b83a1b"
      },
      "source": [
        "input_size_encoder = len(english.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "\n",
        "encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
        "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
        "print(encoder_lstm)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(5893, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2S1D4zzeBzl",
        "outputId": "bf025291-9f29-414b-d3b7-2f0ccde781ce"
      },
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "    \"\"\"\n",
        "    input_size: decoder的輸入，one hot vector 的型式，也就是字典大小，但這次是德文的decoder\n",
        "    embedding_size: embedding dim的大小，表示你希望把字濃縮成幾維的向量\n",
        "    hidden_size: 賦予lstm 隱藏層的神經元數量\n",
        "    num_layers: 想要幾層的lstm\n",
        "    p: dropout的%數\n",
        "    output_size: 最後輸出的長度，也是會表示成字典大小，看哪個德文的詞出現的機率大，這裡input_size = output_size\n",
        "    \"\"\"\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.output_size = output_size\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    # 我們的例子:[7853, 300]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    # [300, 2, 1024]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "    # [1024, 7853] \n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "  \n",
        "  def forward(self, x, hidden_state, cell_state):\n",
        "    \"\"\"\n",
        "    x: [batch_size]\n",
        "    hidden_state: 由encoder傳進來 [26, 32, 1024]\n",
        "    cell_state：由encoder傳進來 [2, 32, 1024]\n",
        "    \"\"\"\n",
        "    # [1, 32]\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    # [1, 32, 300] [1, batch_size, embedding dims(對應nn.LSTM的input_size)]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # outputs [1, 32, 1024] [1, batch_size , hidden_size]\n",
        "    # (hs, cs) [2, 32, 1024] , [2, 32, 1024] [num_layers, batch_size size, hidden_size]\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "\n",
        "    # [1, 32, 7853] [ 1, batch_size , output_size]\n",
        "    predictions = self.fc(outputs)\n",
        "\n",
        "    # [32, 7853] [batch_size , output_size]\n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden_state, cell_state\n",
        "\n",
        "\n",
        "input_size_decoder = len(german.vocab)\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "decoder_dropout = 0.5\n",
        "output_size = len(german.vocab)\n",
        "\n",
        "decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
        "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
        "print(decoder_lstm)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(7853, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=1024, out_features=7853, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK0_iDlmhP3E",
        "outputId": "9d661b03-345f-4b9e-daa0-3943147fa6ef"
      },
      "source": [
        "for batch in train_iterator:\n",
        "  print(batch.src.shape)\n",
        "  print(batch.trg.shape)\n",
        "  break\n",
        "\n",
        "x = batch.trg[1]\n",
        "print(x)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([11, 32])\n",
            "torch.Size([13, 32])\n",
            "tensor([   5,    5,  168, 5561,    8,    5,    5,    8,    8,  105,    8,   76,\n",
            "          18,    5,    5,    8,    5,   18,   18,   39,   36,    5,   18,    8,\n",
            "          18,   18,    5,   18,    5,   18,    8,   43], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1sczoMthajc"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.Encoder_LSTM = Encoder_LSTM\n",
        "    self.Decoder_LSTM = Decoder_LSTM\n",
        "  \n",
        "  def forward(self, source, target, tfr = 0.5):\n",
        "    \"\"\"\n",
        "    source: English [src_len, batch_size]\n",
        "    target: german [src_len, batch_size]\n",
        "    tfr: teacher force ratio\n",
        "    \"\"\"\n",
        "    batch_size = source.shape[1]\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(german.vocab)\n",
        "\n",
        "    # [target_len, 32, 7853]\n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "    # [2, 32, 1024]\n",
        "    hidden_state, cell_state = self.Encoder_LSTM(source)\n",
        "    \n",
        "    # token <SOS> [32] 只有batch_size的長度\n",
        "    x = target[0]\n",
        "\n",
        "    for i in range(1, target_len):\n",
        "      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
        "      outputs[i] = output\n",
        "\n",
        "      # 0th dimension is batch size, 1st dimension is word vocab size\n",
        "      best_guess = output.argmax(1)\n",
        "\n",
        "      # 若使用teacher forcing 就是用原始的input(german)\n",
        "      x = target[i] if random.random() < tfr else best_guess\n",
        "    \n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRkBh06rvp3m"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgBSrtetv57A",
        "outputId": "4c50f750-ecf0-49fb-ad31-32453591f24d"
      },
      "source": [
        "model"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (Encoder_LSTM): EncoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(5893, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (Decoder_LSTM): DecoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(7853, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=1024, out_features=7853, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkMpXTD6v7GV"
      },
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    \"\"\"\n",
        "    model: seq2seq\n",
        "    sentence: English\n",
        "    german: torchtext field\n",
        "    english: torchtext field\n",
        "    \"\"\"\n",
        "    spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text.lower() for token in spacy_eng(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    # 補足sos\n",
        "    tokens.insert(0, english.init_token)\n",
        "    # 補足eos\n",
        "    tokens.append(english.eos_token)\n",
        "    # 轉成id\n",
        "    text_to_indices = [english.vocab.stoi[token] for token in tokens]\n",
        "    # 轉成tensor， 並變成[src_len, 1]，符合encoder格式\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state，不做梯度\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
        "\n",
        "    outputs = [german.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == german.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [german.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "# 用來評估模型的函式: bleu\n",
        "def bleu(data, model, german, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, german, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n",
        "\n",
        "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n",
        "    print('saving')\n",
        "    print()\n",
        "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
        "    torch.save(state, './checkpoint-NMT')\n",
        "    torch.save(model.state_dict(),'./checkpoint-NMT-SD')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZlPQ4g4xvbD",
        "outputId": "bed3dad5-c3f6-4d4d-ab25-acf6d2d8cf76"
      },
      "source": [
        "epoch_loss = 0.0\n",
        "num_epochs = 100\n",
        "best_loss = 999999\n",
        "best_epoch = -1\n",
        "sentence1 = \"two young white males are outside near many bushes\"\n",
        "ts1  = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
        "  model.eval()\n",
        "  translated_sentence1 = translate_sentence(model, sentence1, german, english, device, max_length=50)\n",
        "  print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n",
        "  ts1.append(translated_sentence1)\n",
        "\n",
        "  model.train(True)\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    input = batch.src.to(device)\n",
        "    target = batch.trg.to(device)\n",
        "\n",
        "    # Pass the input and target for model's forward method\n",
        "    output = model(input, target)\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    # Clear the accumulating gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss value for every epoch\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Calculate the gradients for weights & biases using back-propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the gradient value is it exceeds > 1\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    # Update the weights values using the gradients we calculated using bp \n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "    epoch_loss += loss.item()\n",
        "    writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "\n",
        "  if epoch_loss < best_loss:\n",
        "    best_loss = epoch_loss\n",
        "    best_epoch = epoch\n",
        "    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
        "    if ((epoch - best_epoch) >= 10):\n",
        "      print(\"no improvement in 10 epochs, break\")\n",
        "      break\n",
        "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
        "  print()\n",
        "  \n",
        "print(epoch_loss / len(train_iterator))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch - 1 / 100\n",
            "Translated example sentence 1: \n",
            " ['r', 'stöcke', 'sicherheitsausrüstung', 'künstlerischen', 'künstlerischen', 'künstlerischen', 'klub', 'welpen', 'welpen', 'schutzweste', 'ernte', 'weinflasche', 'weißem', 'sonnenschirm', 'sonnenschirm', 'sonnenschirm', 'sonnenschirm', 'euro', 'tennisspielerin', 'lauscht', 'lauscht', 'lauscht', 'armut', 'armut', 'gepolsterter', 'spiegel', 'tennisspielerin', 'schleudert', 'vorlesung', 'sonnenschirm', 'sonnenschirm', 'sonnenschirm', 'sonnenschirm', 'zustand', 'künstlerischen', 'künstlerischen', 'künstlerischen', 'klub', 'klub', 'welpen', 'welpen', 'jim', 'markisen', 'au', 'eyeliner', 'stapel', 'au', 'zustand', 'signiert', 'teddybären']\n",
            "saving\n",
            "\n",
            "Epoch_Loss - 4.366153240203857\n",
            "\n",
            "Epoch - 2 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'männer', 'spielen', 'auf', '.', '<eos>']\n",
            "Epoch_Loss - 2.815384864807129\n",
            "\n",
            "Epoch - 3 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'männer', 'männer', 'in', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 3.7022738456726074\n",
            "\n",
            "Epoch - 4 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'männer', 'stehen', 'nebeneinander', 'und', 'lächeln', '.', '<eos>']\n",
            "Epoch_Loss - 2.897352933883667\n",
            "\n",
            "Epoch - 5 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'männer', 'männer', 'stehen', 'im', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 3.5626718997955322\n",
            "\n",
            "Epoch - 6 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'stehen', 'draußen', 'im', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 3.578320026397705\n",
            "\n",
            "Epoch - 7 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'männer', 'stehen', 'draußen', 'im', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 2.642620086669922\n",
            "\n",
            "Epoch - 8 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'männer', 'männer', 'draußen', 'draußen', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 2.1065287590026855\n",
            "\n",
            "Epoch - 9 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'männer', 'männer', 'draußen', 'draußen', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 1.6936683654785156\n",
            "\n",
            "Epoch - 10 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'stehen', 'draußen', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 2.8481709957122803\n",
            "\n",
            "Epoch - 11 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'stehen', 'draußen', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 1.4223096370697021\n",
            "\n",
            "Epoch - 12 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'stehen', 'draußen', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 1.1722525358200073\n",
            "\n",
            "Epoch - 13 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 1.6032410860061646\n",
            "\n",
            "Epoch - 14 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'draußen', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.532283067703247\n",
            "\n",
            "Epoch - 15 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'stehen', 'draußen', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 2.10859751701355\n",
            "\n",
            "Epoch - 16 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'draußen', 'freien', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.8480479717254639\n",
            "\n",
            "Epoch - 17 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'draußen', 'in', 'der', 'nähe', 'vieler', 'büsche', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.2611075639724731\n",
            "\n",
            "Epoch - 18 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'büsche', 'büsche', 'büsche', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.0306824445724487\n",
            "\n",
            "Epoch - 19 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'vieler', 'vieler', 'vieler', '.', '<eos>']\n",
            "Epoch_Loss - 2.4183099269866943\n",
            "\n",
            "Epoch - 20 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'der', 'nähe', 'von', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 2.111236095428467\n",
            "\n",
            "Epoch - 21 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'stehen', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'vieler', '.', '<eos>']\n",
            "Epoch_Loss - 0.9013781547546387\n",
            "\n",
            "Epoch - 22 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'der', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.1992425918579102\n",
            "\n",
            "Epoch - 23 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'vieler', 'büsche', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.8926079273223877\n",
            "\n",
            "Epoch - 24 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'vieler', 'büsche', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.9196406006813049\n",
            "\n",
            "Epoch - 25 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 2.472933292388916\n",
            "\n",
            "Epoch - 26 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.7854645252227783\n",
            "\n",
            "Epoch - 27 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.9254543781280518\n",
            "\n",
            "Epoch - 28 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.3129318058490753\n",
            "\n",
            "Epoch - 29 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.8683766722679138\n",
            "\n",
            "Epoch - 30 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.106204628944397\n",
            "\n",
            "Epoch - 31 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.2822589874267578\n",
            "\n",
            "Epoch - 32 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'draußen', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.8600748777389526\n",
            "\n",
            "Epoch - 33 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.40529707074165344\n",
            "\n",
            "Epoch - 34 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'vieler', 'nähe', 'vieler', '.', '<eos>']\n",
            "Epoch_Loss - 1.3286832571029663\n",
            "\n",
            "Epoch - 35 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.4389447867870331\n",
            "\n",
            "Epoch - 36 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.26647570729255676\n",
            "\n",
            "Epoch - 37 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.3503889739513397\n",
            "\n",
            "Epoch - 38 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.3529250621795654\n",
            "\n",
            "Epoch - 39 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.3762507736682892\n",
            "\n",
            "Epoch - 40 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.601749062538147\n",
            "\n",
            "Epoch - 41 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.3702077865600586\n",
            "\n",
            "Epoch - 42 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.9379443526268005\n",
            "\n",
            "Epoch - 43 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'draußen', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.7591598629951477\n",
            "\n",
            "Epoch - 44 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.5083383321762085\n",
            "\n",
            "Epoch - 45 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.4621838331222534\n",
            "\n",
            "Epoch - 46 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.2534582018852234\n",
            "\n",
            "Epoch - 47 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'büsche', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.7344356179237366\n",
            "\n",
            "Epoch - 48 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'männer', 'männer', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.2069801092147827\n",
            "\n",
            "Epoch - 49 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.21010105311870575\n",
            "\n",
            "Epoch - 50 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.4128032624721527\n",
            "\n",
            "Epoch - 51 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.19220829010009766\n",
            "\n",
            "Epoch - 52 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.5846454501152039\n",
            "\n",
            "Epoch - 53 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'vieler', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.9741685390472412\n",
            "\n",
            "Epoch - 54 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.9250684976577759\n",
            "\n",
            "Epoch - 55 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.7503383755683899\n",
            "\n",
            "Epoch - 56 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.29486513137817383\n",
            "\n",
            "Epoch - 57 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.44342583417892456\n",
            "\n",
            "Epoch - 58 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 0.3220614790916443\n",
            "\n",
            "Epoch - 59 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.13235068321228027\n",
            "\n",
            "Epoch - 60 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 0.4105619192123413\n",
            "\n",
            "Epoch - 61 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.5684354901313782\n",
            "\n",
            "Epoch - 62 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.5305956602096558\n",
            "\n",
            "Epoch - 63 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.1529615968465805\n",
            "\n",
            "Epoch - 64 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.36120933294296265\n",
            "\n",
            "Epoch - 65 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.22170452773571014\n",
            "\n",
            "Epoch - 66 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.41753876209259033\n",
            "\n",
            "Epoch - 67 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.0231629610061646\n",
            "\n",
            "Epoch - 68 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.8809211254119873\n",
            "\n",
            "Epoch - 69 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.5437954664230347\n",
            "\n",
            "Epoch - 70 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.304815411567688\n",
            "\n",
            "Epoch - 71 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.8355363607406616\n",
            "\n",
            "Epoch - 72 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.4412745535373688\n",
            "\n",
            "Epoch - 73 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'draußen', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.29242590069770813\n",
            "\n",
            "Epoch - 74 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.1891135275363922\n",
            "\n",
            "Epoch - 75 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.4998561441898346\n",
            "\n",
            "Epoch - 76 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.22907599806785583\n",
            "\n",
            "Epoch - 77 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.28281113505363464\n",
            "\n",
            "Epoch - 78 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.5561279058456421\n",
            "\n",
            "Epoch - 79 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.243576318025589\n",
            "\n",
            "Epoch - 80 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.18925774097442627\n",
            "\n",
            "Epoch - 81 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.28369948267936707\n",
            "\n",
            "Epoch - 82 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.32752203941345215\n",
            "\n",
            "Epoch - 83 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.2456725835800171\n",
            "\n",
            "Epoch - 84 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', '.', '<eos>']\n",
            "Epoch_Loss - 0.3146454691886902\n",
            "\n",
            "Epoch - 85 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.8339825868606567\n",
            "\n",
            "Epoch - 86 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.5712729692459106\n",
            "\n",
            "Epoch - 87 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.14248540997505188\n",
            "\n",
            "Epoch - 88 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.11052896082401276\n",
            "\n",
            "Epoch - 89 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.28070151805877686\n",
            "\n",
            "Epoch - 90 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.195419430732727\n",
            "\n",
            "Epoch - 91 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.30506497621536255\n",
            "\n",
            "Epoch - 92 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 1.0054477453231812\n",
            "\n",
            "Epoch - 93 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.2916969954967499\n",
            "\n",
            "Epoch - 94 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.28422147035598755\n",
            "\n",
            "Epoch - 95 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.37373360991477966\n",
            "\n",
            "Epoch - 96 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 0.7726258635520935\n",
            "\n",
            "Epoch - 97 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.1443752646446228\n",
            "\n",
            "Epoch - 98 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', '.', '<eos>']\n",
            "Epoch_Loss - 0.2567068040370941\n",
            "\n",
            "Epoch - 99 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.3575823903083801\n",
            "\n",
            "Epoch - 100 / 100\n",
            "Translated example sentence 1: \n",
            " ['zwei', 'junge', 'weiße', 'männer', 'sind', 'draußen', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            "Epoch_Loss - 0.15920966863632202\n",
            "\n",
            "84.95244390757132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vB-h6Gcybpf"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Ww2KM3FB7S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}